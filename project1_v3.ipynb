{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Airbnb Analysis v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Business understanding\n",
    "\n",
    "\n",
    "##### The main problem is helping a persona (character) choosing a home where he/she is going to pass next vacations based on his/her aspirations.\n",
    "\n",
    "\n",
    "### 2) Data understanding\n",
    "\n",
    "##### It was based on the data downloaded in http://insideairbnb.com/get-the-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all three datasets\n",
    "\n",
    "calendar = pd.read_csv('calendar.csv', sep=',')\n",
    "listings = pd.read_csv('listings.csv', sep=',')\n",
    "reviews = pd.read_csv('reviews.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking a head of each dataframe\n",
    "\n",
    "calendar.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calendar is a dataframe with the asking prices from the houses (any places) offered in airbnb website and also other variables like minimum and maximum nights permited considering the price and adjusted_price registered.\n",
    "##### Listings is a dataframe considering all houses registered in airbnb website with descriptions like a picture, reviews per month neighborhood overview, etc.\n",
    "##### Reviews is a dataframe with all the reviews from people who already have been hosted in those houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NaN values, if found, should be treated as the average price per neighborhood. Considering the analysis is going to be done by average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task to put adjusted price as a workable float.\n",
    "\n",
    "calendar['adjusted_price'] = calendar['adjusted_price'].str.replace('$', '')\n",
    "calendar['adjusted_price'] = calendar['adjusted_price'].str.replace(',', '')\n",
    "calendar['adjusted_price'] = calendar['adjusted_price'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging a new dataframe with the prices and the neighborhood of each house listed.\n",
    "\n",
    "calendar.columns = ['id', 'date', 'availabe', 'price', 'adjusted_price', 'min_nights', 'max_nights']\n",
    "\n",
    "calendar = pd.merge(calendar, listings[['id', \n",
    "                                        'neighbourhood_cleansed', \n",
    "                                        'neighbourhood_group_cleansed']], on='id')\n",
    "calendar.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting NaN values in a dataframe\n",
    "\n",
    "na_df = pd.DataFrame(index  = ['id', \n",
    "                               'date', \n",
    "                               'available', \n",
    "                               'price', \n",
    "                               'adjusted_price', \n",
    "                               'min_nights', \n",
    "                               'max_nights', \n",
    "                               'neighbourhood_cleansed', \n",
    "                               'neighbourhood_group_cleansed'],                     \n",
    "                     data = (calendar['id'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['date'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['availabe'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['price'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['adjusted_price'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['min_nights'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['max_nights'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['neighbourhood_cleansed'].isna().sum()/calendar.shape[0], \n",
    "                            calendar['neighbourhood_group_cleansed'].isna().sum()/calendar.shape[0]), \n",
    "                     columns=['NaN values %'])\n",
    "\n",
    "\n",
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging neighborhood and reviews in a small list to check in NLTK\n",
    "\n",
    "small_listings = pd.merge(reviews, listings[['id', 'neighbourhood_cleansed', 'description']], left_on='listing_id', right_on='id').drop(['id_x', 'id_y'], axis=1)\n",
    "small_listings = small_listings[['neighbourhood_cleansed', 'comments', 'description']]\n",
    "small_listings.dropna(subset=['neighbourhood_cleansed', 'comments'], axis=0)\n",
    "small_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting stopwords and including some stopwords in the list.\n",
    "\n",
    "my_stopwords = stopwords.words('english')\n",
    "my_stopwords.extend(['br', '', 'The', 'for'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting revies per neighborhood and selecting top 20\n",
    "# The more reviews, more \"real\" can be the analysis.\n",
    "\n",
    "listings_top_20 = small_listings.groupby('neighbourhood_cleansed').agg({'comments': 'count'}).sort_values(by='comments', \n",
    "                                                                                        ascending=False).head(20)\n",
    "listings_top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Selecting bigrams \"adjective\" + \"substantive\"\n",
    "# It's gonna take several hours (in my case 6 hours)\n",
    "# That's why I chose to save that in a pickle file\n",
    "\n",
    "%%time\n",
    "\n",
    "import csv\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "neigh_list = list(listings_top_20.index)\n",
    "all_bigrams = {}\n",
    "for neigh in neigh_list:\n",
    "    bigrams = {}\n",
    "    bigram = []\n",
    "    for sentence in small_listings[small_listings['neighbourhood_cleansed'] == neigh].comments:\n",
    "        try:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            for i in range(len(words)):\n",
    "                if nltk.pos_tag(words)[i][1] == 'JJ' and nltk.pos_tag(words)[i+1][1] in ['NN', 'NNS']:\n",
    "                    adjective = WordNetLemmatizer().lemmatize(words[i], pos='a')\n",
    "                    noun = WordNetLemmatizer().lemmatize(words[i+1], pos='n')\n",
    "                    bigram.append((adjective.lower(), noun.lower()))\n",
    "                    bigrams.update({str(neigh): bigram})\n",
    "        except:\n",
    "            None\n",
    "    all_bigrams.update({str(neigh): bigram})\n",
    "    print('Updated new neighborhood: {}' .format(neigh))\n",
    "    neigh = neigh.replace(' ', '_')\n",
    "    neigh = neigh.replace('-', '')\n",
    "    neigh = neigh.replace('/', '')\n",
    "    pd.DataFrame(bigrams).to_csv('bigram_' + str(neigh) + '.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the bigram dict in pickle\n",
    "\n",
    "file_save = open(\"reviews_bigrams_top20.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(all_bigrams, file_save)\n",
    "\n",
    "file_save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading pickle dict\n",
    "\n",
    "reviews_bigrams = open(\"reviews_bigrams.pkl\", \"rb\")\n",
    "\n",
    "all_bigrams = pickle.load(reviews_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the top bigrams\n",
    "\n",
    "def get_top_bigrams(word, how_many, neigh):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function returns a sorted dataframe with bigrams in index and neighborhood in \\\n",
    "    columns showing the frequencies of the bigrams:\n",
    "    \n",
    "        word = the word we want to search in the bigrams collected, \n",
    "        how_many = the size of the list of bigrams we want to look, by descending frequency, \n",
    "        neigh = the neighborhood list in the dataset we want to analyze\n",
    "        \n",
    "        '''\n",
    "\n",
    "    best_bigrams = {}\n",
    "\n",
    "    for i in range(len(all_bigrams.get(neigh))):\n",
    "        if word in all_bigrams.get(neigh)[i]:\n",
    "            bigrama = all_bigrams.get(neigh)[i]\n",
    "            perc = all_bigrams.get(neigh).count(all_bigrams.get(neigh)[i])/len(all_bigrams.get(neigh))\n",
    "            best_bigrams.update({bigrama: round(100*perc, 2)})\n",
    "\n",
    "    best_df = pd.DataFrame(data=best_bigrams.values(), index=best_bigrams.keys(), columns=[str(neigh)])\n",
    "    \n",
    "    return best_df.sort_values(by=str(neigh), ascending=False).head(how_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get neighborhood list descending frequencies by word.\n",
    "\n",
    "def neigh_by_noun(word, neighlist):\n",
    "    \n",
    "    '''\n",
    "    This function returns a dataframe with the 5 most registered bigrams\n",
    "    by all the neighborhoods in a list.\n",
    "    \n",
    "    word = the word in a bigram we want to search for\n",
    "    neighlist = list of neighborhoods in the dataframe returned    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for neigh in neighlist:\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            df = pd.concat([df, get_top_bigrams(word, 5, neigh)], axis=1).fillna(0)\n",
    "        \n",
    "        except:\n",
    "        \n",
    "            None\n",
    "    \n",
    "    return df.nlargest(5, df.columns, keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the neigh list I choose in the problem\n",
    "\n",
    "neighlist = ['Atlantic', \n",
    "'South Lake Union', \n",
    "'Eastlake', \n",
    "'Stevens', \n",
    "'Green Lake', \n",
    "'Wallingford', \n",
    "'Fremont', \n",
    "'Loyal Heights', \n",
    "'North Deridge', \n",
    "'Mount Baker', \n",
    "'North Beacon Hill', \n",
    "'Columbia City', \n",
    "'Brighton', \n",
    "'Adams', \n",
    "'Belltown', \n",
    "'Broadway', \n",
    "'Interbay', \n",
    "'Minor', \n",
    "'Seward Park', \n",
    "'Pioneer Square', \n",
    "'Ravenna', \n",
    "'Leschi', \n",
    "'University District', \n",
    "'Greenwood', \n",
    "'Fairmount Park', \n",
    "'Mid-Beacon Hill', \n",
    "'Roosevelt', \n",
    "'Yesler Terrace', \n",
    "'North College Park']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1) How do average prices in the whole Seattle behaves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting raw ask prices\n",
    "\n",
    "calendar.groupby('date').mean().adjusted_price.plot(figsize=(16,5), linewidth=2, color='tab:blue')\n",
    "plt.xlabel('')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ylabel('Price US$/day', fontsize=14)\n",
    "plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can clearly see average price oscilates in a short-term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting a seven day moving average, the short-term spikes vanish, so we can tell it has a week seasonality\n",
    "##### 4.2) What about the behavior of the prices in a more long-term?\n",
    "##### It spikes in Thanksgiving and Christmas and the highest values are found in summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting seven days moving average in price\n",
    "\n",
    "hist_prices = calendar.groupby('date').agg({'adjusted_price': 'mean'}).rolling(window = 7).agg({'adjusted_price': ('mean', 'std')})\n",
    "\n",
    "hist_prices['adjusted_price']['mean'].plot(figsize=(18,6), linewidth=2, color='tab:blue')\n",
    "plt.xlabel('')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ylabel('Price US$', fontsize=14)\n",
    "plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3) How the average prices are spread in geographically? What are the most and least expensive neighborhoods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering neighboorhoods with at least 30 reviews and plotting average price in the whole dataset\n",
    "\n",
    "count_neigh = listings['neighbourhood_cleansed'].value_counts()\n",
    "min_count = 30\n",
    "\n",
    "filtered = list(count_neigh[count_neigh > min_count].index)\n",
    "\n",
    "cal_filt = calendar['neighbourhood_cleansed'].isin(filtered)\n",
    "\n",
    "neigh_prices = calendar[cal_filt].groupby('neighbourhood_cleansed').agg({'adjusted_price':\n",
    "                                                                'mean'}).sort_values(by='adjusted_price', \n",
    "                                                                                     ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "barplot = neigh_prices.plot(kind='bar',figsize=(19,6), color='goldenrod')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['Southeast Magnolia'])[0]].set_facecolor('r')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['West Queen Anne'])[0]].set_facecolor('r')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['Central Business District'])[0]].set_facecolor('r')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['North College Park'])[0]].set_facecolor('g')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['Roosevelt'])[0]].set_facecolor('g')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['Yesler Terrace'])[0]].set_facecolor('g')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['Mid-Beacon Hill'])[0]].set_facecolor('g')\n",
    "barplot.patches[neigh_prices.index.get_indexer(['Fairmount Park'])[0]].set_facecolor('g')\n",
    "\n",
    "\n",
    "fontsize=16\n",
    "\n",
    "plt.xticks(fontsize=fontsize);\n",
    "plt.xlabel('Neighbourhood', fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize);\n",
    "plt.ylabel('Price U$/day', fontsize=fontsize)\n",
    "plt.legend('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most expensive: Southeast Magnolia, West Queen Anne and Central Business District.\n",
    "##### The least expensive: North Coilege Park, Roosevelt, Yesler Terrace, Mid-Beacon Hill and Fairmount Park."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4) How can the reviews help us to choose a neighborhood and book our stay? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the bigram by neighborhood in a pandas dataframe\n",
    "\n",
    "occurrences_dt = neigh_by_noun('neighborhood', neighlist)\n",
    "occurrences_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In bigram_wanted we can change the words to filter whatever we want.\n",
    "\n",
    "bigram_wanted = ('quiet', 'neighborhood')\n",
    "\n",
    "occurrences_dt.loc[bigram_wanted].sort_values(ascending=False).plot(kind='bar', figsize=(18,6))\n",
    "\n",
    "plt.xticks(fontsize=fontsize);\n",
    "plt.xlabel('Neighbourhood', fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize);\n",
    "plt.ylabel('% of '+str(bigram_wanted), fontsize=fontsize)\n",
    "plt.legend('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Explaining key insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Considering each pass seen in this notebook:\n",
    "##### - The ask prices are higher as the weekend comes, so there's a week seasonality in this dataset.\n",
    "##### - Prices are higher at certain dates as Christmas and Thanksgiving, get falling in the beginning of the year and hit the top at the summer.\n",
    "##### - We can check the prices of the houses, but considering we don't have the area of each place, we still can't normalize the price for neighborhood as dollar per square feet.\n",
    "##### - Average prices per neighborhood vary from less than 80 dollars to over 250 dollars per night.\n",
    "##### - According to reviews, we found that the neighborhood most cited as \"quite\" is Loyal Heights followed by Ravenna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
